{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","mount_file_id":"15USOz6KrY1lY9OoF4mC5qsWjeIizVr9x","authorship_tag":"ABX9TyMkbMYWAiEu+Ic8zMUmMyQa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["\"\"\"\n","PHASE 2: DQN Strategic Layer with Application-Aware Learning\n","Uses drl_states_actions.npz and application_profiles.csv from Phase 1\n","\"\"\"\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import deque\n","import random\n","import json\n","import pickle\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","# ============================================================================\n","# SECTION 1: Load Phase 1 Outputs (CRITICAL CHANGE)\n","# ============================================================================\n","\n","print(\"=\"*80)\n","print(\"Loading Phase 1 Processed Data\")\n","print(\"=\"*80)\n","\n","DATA_PATH = '/content/drive/MyDrive/mythesis/rohit-thesis/datasets/processed'\n","\n","# Load pre-computed DRL states\n","print(\"\\n[1/5] Loading DRL state representations...\")\n","drl_data = np.load(f'{DATA_PATH}/drl_states_actions.npz', allow_pickle=True)\n","\n","strategic_states = drl_data['strategic_states']  # (1.8M, 10)\n","action_spaces = drl_data['action_spaces'].item()  # Action definitions\n","\n","print(f\" Strategic states shape: {strategic_states.shape}\")\n","print(f\" Action spaces: {action_spaces['strategic']}\")\n","\n","# Load application profiles\n","print(\"\\n[2/5] Loading application profiles...\")\n","app_profiles = pd.read_csv(f'{DATA_PATH}/application_profiles.csv')\n","\n","print(f\" Application profiles: {len(app_profiles)} applications\")\n","print(f\"\\n  Workload distribution:\")\n","for wtype, count in app_profiles['workload_type'].value_counts().items():\n","    print(f\"    {wtype:15s}: {count:>3} apps\")\n","\n","# Load metadata for splits\n","print(\"\\n[3/5] Loading train/val/test splits...\")\n","train_df = pd.read_parquet(f'{DATA_PATH}/train_data.parquet')\n","val_df = pd.read_parquet(f'{DATA_PATH}/val_data.parquet')\n","test_df = pd.read_parquet(f'{DATA_PATH}/test_data.parquet')\n","\n","print(f\" Train samples: {len(train_df):,}\")\n","print(f\" Val samples: {len(val_df):,}\")\n","print(f\" Test samples: {len(test_df):,}\")\n","\n","# Split strategic states by dataset split\n","strategic_states_train = strategic_states[:len(train_df)]\n","strategic_states_val = strategic_states[len(train_df):len(train_df)+len(val_df)]\n","strategic_states_test = strategic_states[len(train_df)+len(val_df):]\n","\n","# Load scaler and metadata\n","print(\"\\n[4/5] Loading feature scaler...\")\n","with open(f'{DATA_PATH}/robust_scaler.pkl', 'rb') as f:\n","    scaler = pickle.load(f)\n","\n","print(\"\\n[5/5] Loading metadata...\")\n","with open(f'{DATA_PATH}/metadata.json', 'r') as f:\n","    metadata = json.load(f)\n","\n","print(f\" DRL Config: {metadata['drl_config']}\")\n","\n","# Create application profile lookup dictionary\n","app_profile_dict = app_profiles.set_index('app').to_dict('index')\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"Phase 1 Data Loaded Successfully\")\n","print(\"=\"*80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOoxt6e1StiN","executionInfo":{"status":"ok","timestamp":1762840138811,"user_tz":-330,"elapsed":839,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}},"outputId":"43641b18-8247-4c0e-d8d7-2cedf1971207"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","Loading Phase 1 Processed Data\n","================================================================================\n","\n","[1/5] Loading DRL state representations...\n"," Strategic states shape: (1264946, 10)\n"," Action spaces: {'cloud_providers': ['AWS', 'Azure', 'GCP'], 'action_space_size': 3}\n","\n","[2/5] Loading application profiles...\n"," Application profiles: 119 applications\n","\n","  Workload distribution:\n","    standard       :  75 apps\n","    bursty         :  44 apps\n","\n","[3/5] Loading train/val/test splits...\n"," Train samples: 1,264,946\n"," Val samples: 271,060\n"," Test samples: 271,061\n","\n","[4/5] Loading feature scaler...\n","\n","[5/5] Loading metadata...\n"," DRL Config: {'strategic_state_dim': 10, 'tactical_state_dim': 7, 'operational_state_dim': 5, 'strategic_actions': 3, 'tactical_actions': 24, 'operational_actions': 3, 'reward_weights': {'alpha': 0.4, 'beta': 0.4, 'gamma': 0.2}, 'sla_penalty': -5.0}\n","\n","================================================================================\n","Phase 1 Data Loaded Successfully\n","================================================================================\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 2: Enhanced State Representation\n","# ============================================================================\n","\n","def create_enhanced_state(strategic_state, row, app_profile_dict):\n","    \"\"\"\n","    Combine strategic state with application-level context\n","\n","    Args:\n","        strategic_state: Pre-computed strategic features (10-dim)\n","        row: Current invocation data\n","        app_profile_dict: Application profile lookup\n","\n","    Returns:\n","        enhanced_state: Combined state vector (14-dim)\n","    \"\"\"\n","    app_id = row['app']\n","    app_profile = app_profile_dict.get(app_id, {})\n","\n","    # Application context features (4-dim)\n","    app_context = np.array([\n","        app_profile.get('cold_start_rate', 0.0),      # Cold start likelihood\n","        app_profile.get('sla_violation_rate', 0.0),   # SLA risk\n","        app_profile.get('avg_invocation_rate', 0.0) / 100.0,  # Normalized traffic\n","        1.0 if app_profile.get('workload_type') == 'bursty' else 0.0  # Burst indicator\n","    ], dtype=np.float32)\n","\n","    # Concatenate: [strategic_state (10) | app_context (4)] = 14-dim\n","    enhanced_state = np.concatenate([strategic_state, app_context])\n","\n","    return enhanced_state\n","\n"],"metadata":{"id":"e11RfypgTA10","executionInfo":{"status":"ok","timestamp":1762840143955,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 3: Application-Aware Multi-Cloud Environment\n","# ============================================================================\n","\n","class AppAwareMultiCloudEnv:\n","    \"\"\"\n","    Enhanced environment that uses Phase 1 outputs:\n","    - Pre-computed strategic states\n","    - Application profile context\n","    - Workload-aware reward shaping\n","    \"\"\"\n","\n","    def __init__(self, strategic_states, data_df, app_profile_dict):\n","        self.strategic_states = strategic_states\n","        self.data_df = data_df\n","        self.app_profile_dict = app_profile_dict\n","\n","        self.state_dim = 14  # 10 strategic + 4 app context\n","        self.action_dim = 3  # AWS, Azure, GCP\n","\n","        # Cloud provider mapping\n","        self.providers = ['AWS', 'Azure', 'GCP']\n","\n","        # Reward weights (from metadata)\n","        self.alpha = 0.4  # Cost weight\n","        self.beta = 0.4   # Performance weight\n","        self.gamma = 0.2  # Carbon weight\n","        self.sla_penalty = 5.0\n","\n","    def reset(self, idx):\n","        \"\"\"\n","        Initialize state for a specific invocation\n","\n","        Args:\n","            idx: Index in the dataset\n","\n","        Returns:\n","            state: Enhanced state vector (14-dim)\n","            row: Data for the invocation\n","        \"\"\"\n","        row = self.data_df.iloc[idx]\n","\n","        # Get pre-computed strategic state\n","        strategic_state = self.strategic_states[idx]\n","\n","        # Create enhanced state with app context\n","        state = create_enhanced_state(strategic_state, row, self.app_profile_dict)\n","\n","        return state, row\n","\n","    def step(self, action, row):\n","        \"\"\"\n","        Execute action and compute application-aware reward\n","\n","        Args:\n","            action: Cloud provider selection (0=AWS, 1=Azure, 2=GCP)\n","            row: Current invocation data\n","\n","        Returns:\n","            reward: Application-aware reward\n","            done: Episode termination flag\n","        \"\"\"\n","        # Get actual outcomes from data (simulated deployment)\n","        cost = row['compute_cost']\n","        latency = row['total_latency_ms']\n","        carbon = row['carbon_footprint_g']\n","        sla_violated = row['sla_violation']\n","\n","        # Normalize rewards (0-1 range)\n","        cost_reward = 1.0 - min(cost / 1.0, 1.0)  # Lower cost = higher reward\n","        perf_reward = 1.0 - min(latency / 1000.0, 1.0)  # Lower latency = higher reward\n","        carbon_reward = 1.0 - min(carbon / 100.0, 1.0)  # Lower carbon = higher reward\n","\n","        # Application-aware reward shaping\n","        app_id = row['app']\n","        app_profile = self.app_profile_dict.get(app_id, {})\n","\n","        # Penalty for cold starts on bursty applications\n","        if app_profile.get('workload_type') == 'bursty' and row.get('is_cold_start', False):\n","            perf_reward -= 0.2\n","\n","        # Bonus for maintaining SLA on high-risk applications\n","        if app_profile.get('sla_violation_rate', 0.0) > 0.1:\n","            if not sla_violated:\n","                perf_reward += 0.1  # Bonus for SLA compliance\n","\n","        # Multi-objective reward\n","        reward = (self.alpha * cost_reward +\n","                 self.beta * perf_reward +\n","                 self.gamma * carbon_reward)\n","\n","        # SLA penalty (asymmetric loss)\n","        if sla_violated:\n","            reward -= self.sla_penalty\n","\n","        return reward, False  # Never truly \"done\" in this setup"],"metadata":{"id":"IPTxYWGFTEsI","executionInfo":{"status":"ok","timestamp":1762840146580,"user_tz":-330,"elapsed":4,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["\n","# ============================================================================\n","# SECTION 4: Enhanced DQN Network Architecture\n","# ============================================================================\n","\n","class EnhancedDQNetwork(nn.Module):\n","    \"\"\"\n","    Enhanced DQN with separate encoders for strategic and application context\n","\n","    Architecture:\n","        Strategic Encoder: 10 -> 64\n","        App Context Encoder: 4 -> 32\n","        Fusion Network: 96 -> 128 -> 64 -> 3\n","    \"\"\"\n","\n","    def __init__(self, state_size=14, action_size=3, hidden_sizes=[64, 32, 128]):\n","        super(EnhancedDQNetwork, self).__init__()\n","\n","        # Strategic encoder (10 -> 64)\n","        self.strategic_encoder = nn.Sequential(\n","            nn.Linear(10, hidden_sizes[0]),\n","            nn.ReLU(),\n","            nn.LayerNorm(hidden_sizes[0]),  # ‚Üê Changed from BatchNorm1d\n","            nn.Dropout(0.1)\n","        )\n","\n","        # App context encoder (4 -> 32)\n","        self.app_context_encoder = nn.Sequential(\n","            nn.Linear(4, hidden_sizes[1]),\n","            nn.ReLU(),\n","            nn.LayerNorm(hidden_sizes[1]),  # ‚Üê Changed from BatchNorm1d\n","            nn.Dropout(0.1)\n","        )\n","\n","        # Fusion network (96 -> 3)\n","        fusion_input_size = hidden_sizes[0] + hidden_sizes[1]\n","        self.fusion = nn.Sequential(\n","            nn.Linear(fusion_input_size, hidden_sizes[2]),\n","            nn.ReLU(),\n","            nn.LayerNorm(hidden_sizes[2]),  # ‚Üê Changed from BatchNorm1d\n","            nn.Dropout(0.2),\n","            nn.Linear(hidden_sizes[2], 64),\n","            nn.ReLU(),\n","            nn.Linear(64, action_size)\n","        )\n","\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n","                nn.init.constant_(module.bias, 0.01)\n","\n","    def forward(self, state):\n","        strategic = state[:, :10]\n","        app_context = state[:, 10:]\n","\n","        strategic_emb = self.strategic_encoder(strategic)\n","        app_emb = self.app_context_encoder(app_context)\n","\n","        combined = torch.cat([strategic_emb, app_emb], dim=1)\n","        q_values = self.fusion(combined)\n","\n","        return q_values"],"metadata":{"id":"5BnySXt6TNUD","executionInfo":{"status":"ok","timestamp":1762840150903,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# SECTION 5: DQN Agent with Experience Replay\n","# ============================================================================\n","\n","class DQNAgent:\n","    \"\"\"\n","    DQN Agent with application-aware learning\n","    \"\"\"\n","\n","    def __init__(self, state_size=14, action_size=3, learning_rate=0.0001,  # ‚Üê Lower LR\n","                 gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=10000,\n","                 buffer_size=100000, batch_size=64, target_update_freq=1000):\n","\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.gamma = gamma\n","\n","        # Epsilon-greedy parameters\n","        self.epsilon = epsilon\n","        self.epsilon_min = epsilon_min\n","        self.epsilon_decay = epsilon_decay\n","        self.steps = 0\n","\n","        # Experience replay\n","        self.memory = deque(maxlen=buffer_size)\n","        self.batch_size = batch_size\n","\n","        # Networks\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.q_network = EnhancedDQNetwork(state_size, action_size).to(self.device)\n","        self.target_network = EnhancedDQNetwork(state_size, action_size).to(self.device)\n","        self.target_network.load_state_dict(self.q_network.state_dict())\n","\n","        # Optimizer with lower learning rate\n","        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n","        self.criterion = nn.SmoothL1Loss(beta=1.0)  # Huber loss\n","\n","        # Target network update\n","        self.target_update_freq = target_update_freq\n","\n","        # Tracking\n","        self.epsilon_history = []\n","        self.loss_history = []\n","        self.nan_count = 0\n","\n","    def act(self, state, training=True):\n","        \"\"\"Epsilon-greedy action selection\"\"\"\n","        if training and random.random() < self.epsilon:\n","            return random.randrange(self.action_size)\n","\n","        self.q_network.eval()\n","        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n","\n","        with torch.no_grad():\n","            q_values = self.q_network(state)\n","\n","        self.q_network.train()\n","        return q_values.argmax().item()\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        \"\"\"Store transition with validity check\"\"\"\n","        # Check for NaN/Inf before storing\n","        if np.isnan(state).any() or np.isnan(next_state).any() or np.isnan(reward):\n","            return  # Skip invalid transitions\n","\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def train_step(self):\n","        \"\"\"Perform one training step with NaN protection\"\"\"\n","        if len(self.memory) < self.batch_size:\n","            return None\n","\n","        # Sample batch\n","        batch = random.sample(self.memory, self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*batch)\n","\n","        # Convert to tensors\n","        states = torch.FloatTensor(np.array(states)).to(self.device)\n","        actions = torch.LongTensor(actions).to(self.device)\n","        rewards = torch.FloatTensor(rewards).to(self.device)\n","        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)\n","        dones = torch.FloatTensor(dones).to(self.device)\n","\n","        # Check for NaN/Inf in inputs\n","        if torch.isnan(states).any() or torch.isinf(states).any():\n","            self.nan_count += 1\n","            return None\n","\n","        # Ensure network is in train mode\n","        self.q_network.train()\n","\n","        # Current Q-values\n","        current_q = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","\n","        # Check Q-values\n","        if torch.isnan(current_q).any() or torch.isinf(current_q).any():\n","            print(f\" NaN/Inf in Q-values at step {self.steps}\")\n","            self.nan_count += 1\n","            return None\n","\n","        # Target Q-values\n","        self.target_network.eval()\n","        with torch.no_grad():\n","            next_q = self.target_network(next_states).max(1)[0]\n","            target_q = rewards + (1 - dones) * self.gamma * next_q\n","\n","        # Clamp targets to prevent explosion\n","        target_q = torch.clamp(target_q, -10.0, 10.0)\n","\n","        # Compute loss\n","        loss = self.criterion(current_q, target_q)\n","\n","        # Check loss\n","        if torch.isnan(loss) or torch.isinf(loss):\n","            print(f\" NaN/Inf loss at step {self.steps}\")\n","            self.nan_count += 1\n","            return None\n","\n","        # Backpropagation\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","\n","        # Aggressive gradient clipping\n","        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 0.5)\n","\n","        self.optimizer.step()\n","\n","        self.loss_history.append(loss.item())\n","\n","        # Update target network\n","        self.steps += 1\n","        if self.steps % self.target_update_freq == 0:\n","            self.target_network.load_state_dict(self.q_network.state_dict())\n","\n","        # Decay epsilon\n","        self.epsilon = max(self.epsilon_min,\n","                          self.epsilon_min + (1.0 - self.epsilon_min) *\n","                          np.exp(-1.0 * self.steps / self.epsilon_decay))\n","        self.epsilon_history.append(self.epsilon)\n","\n","        return loss.item()\n"],"metadata":{"id":"ne8tsqQaTRos","executionInfo":{"status":"ok","timestamp":1762840154025,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# ============================================================================\n","# Re-extract Strategic States from Full Dataset\n","# ============================================================================\n","\n","print(\"=\"*80)\n","print(\"RE-EXTRACTING STRATEGIC STATES FROM FULL DATASET\")\n","print(\"=\"*80)\n","\n","print(\"\\n[Problem Identified]\")\n","print(\"  Phase 1 only extracted states from train_data, not full dataset!\")\n","print(\"  drl_states_actions.npz only has training data (1,264,946 samples)\")\n","print(\"  But we need ALL data: train + val + test (1,807,067 samples)\")\n","\n","print(\"\\n[Solution]\")\n","print(\"  Re-extract strategic states from train_df + val_df + test_df\")\n","\n","# Define strategic state features (same as Phase 1)\n","strategic_state_features = [\n","    'hour', 'day_of_week', 'is_weekend', 'is_business_hours',\n","    'invocation_rate', 'is_bursty',\n","    'avg_duration', 'avg_cost', 'avg_carbon',\n","    'memory_mb'\n","]\n","\n","print(f\"\\n[Step 1] Concatenating all dataframes...\")\n","print(f\"  Train: {len(train_df):,} samples\")\n","print(f\"  Val:   {len(val_df):,} samples\")\n","print(f\"  Test:  {len(test_df):,} samples\")\n","\n","# Concatenate in order: train -> val -> test\n","full_df = pd.concat([train_df, val_df, test_df], axis=0, ignore_index=True)\n","print(f\"  Total: {len(full_df):,} samples\")\n","\n","# Check if all features exist\n","print(f\"\\n[Step 2] Checking features...\")\n","missing_features = [f for f in strategic_state_features if f not in full_df.columns]\n","if missing_features:\n","    print(f\"  Missing features: {missing_features}\")\n","    print(f\"  Available columns: {list(full_df.columns)}\")\n","else:\n","    print(f\"  ‚úì All strategic features present\")\n","\n","# Extract strategic states from FULL dataset\n","print(f\"\\n[Step 3] Extracting strategic states from full dataset...\")\n","strategic_states_full = full_df[strategic_state_features].values\n","\n","print(f\"  ‚úì Extracted: {strategic_states_full.shape}\")\n","print(f\"     Features: {strategic_state_features}\")\n","\n","# Verify no NaN values\n","nan_count = np.isnan(strategic_states_full).sum()\n","if nan_count > 0:\n","    print(f\"     Warning: {nan_count} NaN values found\")\n","    print(f\"     Filling NaN with 0...\")\n","    strategic_states_full = np.nan_to_num(strategic_states_full, nan=0.0)\n","else:\n","    print(f\"  ‚úì No NaN values\")\n","\n","# Split into train/val/test\n","print(f\"\\n[Step 4] Splitting into train/val/test...\")\n","train_size = len(train_df)\n","val_size = len(val_df)\n","test_size = len(test_df)\n","\n","strategic_states_train = strategic_states_full[:train_size]\n","strategic_states_val = strategic_states_full[train_size:train_size+val_size]\n","strategic_states_test = strategic_states_full[train_size+val_size:]\n","\n","print(f\"  Train: {strategic_states_train.shape}\")\n","print(f\"  Val:   {strategic_states_val.shape}\")\n","print(f\"  Test:  {strategic_states_test.shape}\")\n","\n","# Verify splits\n","assert strategic_states_train.shape[0] == len(train_df), \"Train size mismatch!\"\n","assert strategic_states_val.shape[0] == len(val_df), \"Val size mismatch!\"\n","assert strategic_states_test.shape[0] == len(test_df), \"Test size mismatch!\"\n","print(f\"  ‚úì All splits verified!\")\n","\n","# Recreate environments\n","print(f\"\\n[Step 5] Recreating environments...\")\n","\n","env = AppAwareMultiCloudEnv(\n","    strategic_states=strategic_states_train,\n","    data_df=train_df,\n","    app_profile_dict=app_profile_dict\n",")\n","\n","val_env = AppAwareMultiCloudEnv(\n","    strategic_states=strategic_states_val,\n","    data_df=val_df,\n","    app_profile_dict=app_profile_dict\n",")\n","\n","print(f\"  ‚úì Training env: {len(env.strategic_states):,} samples\")\n","print(f\"  ‚úì Validation env: {len(val_env.strategic_states):,} samples\")\n","\n","# Test both environments\n","print(f\"\\n[Step 6] Testing environments...\")\n","test_state_train, _ = env.reset(0)\n","test_state_val, _ = val_env.reset(0)\n","print(f\"  ‚úì Training env test: {test_state_train.shape}\")\n","print(f\"  ‚úì Validation env test: {test_state_val.shape}\")\n","\n","# Show sample values\n","print(f\"\\n[Step 7] Sample strategic states:\")\n","print(f\"  Train sample [0]: {strategic_states_train[0]}\")\n","print(f\"  Val sample [0]:   {strategic_states_val[0]}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"FIX COMPLETE!\")\n","print(\"=\"*80)\n","\n","print(\"\\nSummary:\")\n","print(f\"  ‚Ä¢ Re-extracted strategic states from FULL dataset\")\n","print(f\"  ‚Ä¢ Training env: {len(env.strategic_states):,} samples\")\n","print(f\"  ‚Ä¢ Validation env: {len(val_env.strategic_states):,} samples\")\n","print(f\"  ‚Ä¢ Both environments tested and working!\")\n","\n","print(\"\\nüéØ You can now restart your training loop!\")\n","\n","# Optional: Save the corrected drl_states_actions.npz for future use\n","\n","#Save corrected DRL states\n","drl_data_corrected = {\n","    'strategic_states': strategic_states_full,\n","    'action_spaces': action_spaces\n","}\n","np.savez_compressed(f'{DATA_PATH}/drl_states_actions_CORRECTED.npz', **drl_data_corrected)\n","print(\"‚úì Saved corrected states to drl_states_actions_CORRECTED.npz\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R8yqXqjmgmGX","executionInfo":{"status":"ok","timestamp":1762841992479,"user_tz":-330,"elapsed":2801,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}},"outputId":"c95abe22-f8d5-4318-8942-dfc24149d7bc"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","RE-EXTRACTING STRATEGIC STATES FROM FULL DATASET\n","================================================================================\n","\n","[Problem Identified]\n","  Phase 1 only extracted states from train_data, not full dataset!\n","  drl_states_actions.npz only has training data (1,264,946 samples)\n","  But we need ALL data: train + val + test (1,807,067 samples)\n","\n","[Solution]\n","  Re-extract strategic states from train_df + val_df + test_df\n","\n","[Step 1] Concatenating all dataframes...\n","  Train: 1,264,946 samples\n","  Val:   271,060 samples\n","  Test:  271,061 samples\n","  Total: 1,807,067 samples\n","\n","[Step 2] Checking features...\n","  ‚úì All strategic features present\n","\n","[Step 3] Extracting strategic states from full dataset...\n","  ‚úì Extracted: (1807067, 10)\n","     Features: ['hour', 'day_of_week', 'is_weekend', 'is_business_hours', 'invocation_rate', 'is_bursty', 'avg_duration', 'avg_cost', 'avg_carbon', 'memory_mb']\n","     Warning: 424 NaN values found\n","     Filling NaN with 0...\n","\n","[Step 4] Splitting into train/val/test...\n","  Train: (1264946, 10)\n","  Val:   (271060, 10)\n","  Test:  (271061, 10)\n","  ‚úì All splits verified!\n","\n","[Step 5] Recreating environments...\n","  ‚úì Training env: 1,264,946 samples\n","  ‚úì Validation env: 271,060 samples\n","\n","[Step 6] Testing environments...\n","  ‚úì Training env test: (14,)\n","  ‚úì Validation env test: (14,)\n","\n","[Step 7] Sample strategic states:\n","  Train sample [0]: [0.00000000e+00 6.00000000e+00 1.00000000e+00 0.00000000e+00\n"," 0.00000000e+00 0.00000000e+00 3.37690426e-01 4.72795679e-04\n"," 5.00007883e-01 2.00000000e+00]\n","  Val sample [0]:   [ 4.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n","  4.04146975e-01  0.00000000e+00 -8.73395034e-02  4.52985893e-04\n","  5.00000001e-01 -1.42857143e-01]\n","\n","================================================================================\n","FIX COMPLETE!\n","================================================================================\n","\n","Summary:\n","  ‚Ä¢ Re-extracted strategic states from FULL dataset\n","  ‚Ä¢ Training env: 1,264,946 samples\n","  ‚Ä¢ Validation env: 271,060 samples\n","  ‚Ä¢ Both environments tested and working!\n","\n","üéØ You can now restart your training loop!\n","‚úì Saved corrected states to drl_states_actions_CORRECTED.npz\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3HcmxDPHSd8C","executionInfo":{"status":"ok","timestamp":1762844900700,"user_tz":-330,"elapsed":2739072,"user":{"displayName":"Vistronics India","userId":"16074982155385528679"}},"outputId":"7fc80983-2418-4775-db7e-743b49d54e09"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Agent initialized on device: cuda\n"," Network parameters: 22,179\n","\n"," Training Configuration:\n","   Episodes: 50\n","   Samples per episode: 10,000\n","   Validation frequency: every 5 episodes\n","\n","================================================================================\n","Starting Training...\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["Episode 1/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:48<00:00, 207.87it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9678 | Loss: 0.0777 | Œµ: 0.3765\n"]},{"output_type":"stream","name":"stderr","text":["Episode 2/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:51<00:00, 194.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9746 | Loss: 0.0461 | Œµ: 0.1448\n"]},{"output_type":"stream","name":"stderr","text":["Episode 3/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:52<00:00, 189.19it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9659 | Loss: 0.0380 | Œµ: 0.0596\n"]},{"output_type":"stream","name":"stderr","text":["Episode 4/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:53<00:00, 187.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9634 | Loss: 0.0336 | Œµ: 0.0282\n"]},{"output_type":"stream","name":"stderr","text":["Episode 5/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:53<00:00, 185.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9662 | Loss: 0.0291 | Œµ: 0.0167\n","  Validation Reward: 0.9513\n","  New best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Episode 6/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 181.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9659 | Loss: 0.0281 | Œµ: 0.0125\n"]},{"output_type":"stream","name":"stderr","text":["Episode 7/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9652 | Loss: 0.0271 | Œµ: 0.0109\n"]},{"output_type":"stream","name":"stderr","text":["Episode 8/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9611 | Loss: 0.0270 | Œµ: 0.0103\n"]},{"output_type":"stream","name":"stderr","text":["Episode 9/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9637 | Loss: 0.0268 | Œµ: 0.0101\n"]},{"output_type":"stream","name":"stderr","text":["Episode 10/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 180.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9691 | Loss: 0.0264 | Œµ: 0.0100\n","  Validation Reward: 0.9708\n","  New best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Episode 11/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 181.86it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9637 | Loss: 0.0262 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 12/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9604 | Loss: 0.0274 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 13/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9663 | Loss: 0.0282 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 14/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9703 | Loss: 0.0275 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 15/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 180.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9713 | Loss: 0.0271 | Œµ: 0.0100\n","  Validation Reward: 0.9601\n"]},{"output_type":"stream","name":"stderr","text":["Episode 16/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 183.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9708 | Loss: 0.0258 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 17/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9644 | Loss: 0.0259 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 18/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9638 | Loss: 0.0258 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 19/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 183.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9675 | Loss: 0.0261 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 20/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 183.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9654 | Loss: 0.0258 | Œµ: 0.0100\n","  Validation Reward: 0.9628\n"]},{"output_type":"stream","name":"stderr","text":["Episode 21/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9678 | Loss: 0.0254 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 22/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 185.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9642 | Loss: 0.0251 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 23/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9665 | Loss: 0.0252 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 24/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.65it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9657 | Loss: 0.0248 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 25/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 183.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9674 | Loss: 0.0240 | Œµ: 0.0100\n","  Validation Reward: 0.9537\n"]},{"output_type":"stream","name":"stderr","text":["Episode 26/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 183.60it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9663 | Loss: 0.0246 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 27/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.22it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9584 | Loss: 0.0261 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 28/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 185.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9691 | Loss: 0.0251 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 29/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 185.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9640 | Loss: 0.0251 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 30/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 183.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9634 | Loss: 0.0254 | Œµ: 0.0100\n","  Validation Reward: 0.9567\n"]},{"output_type":"stream","name":"stderr","text":["Episode 31/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.02it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9663 | Loss: 0.0266 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 32/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9679 | Loss: 0.0252 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 33/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 180.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9633 | Loss: 0.0251 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 34/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 179.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9733 | Loss: 0.0250 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 35/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 180.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9739 | Loss: 0.0251 | Œµ: 0.0100\n","  Validation Reward: 0.9601\n"]},{"output_type":"stream","name":"stderr","text":["Episode 36/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 180.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9679 | Loss: 0.0238 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 37/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 178.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9628 | Loss: 0.0237 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 38/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9673 | Loss: 0.0249 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 39/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9682 | Loss: 0.0238 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 40/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.94it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9647 | Loss: 0.0240 | Œµ: 0.0100\n","  Validation Reward: 0.9581\n"]},{"output_type":"stream","name":"stderr","text":["Episode 41/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9698 | Loss: 0.0247 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 42/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9640 | Loss: 0.0238 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 43/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 181.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9630 | Loss: 0.0240 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 44/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 181.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9571 | Loss: 0.0250 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 45/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9600 | Loss: 0.0256 | Œµ: 0.0100\n","  Validation Reward: 0.9705\n"]},{"output_type":"stream","name":"stderr","text":["Episode 46/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 184.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9716 | Loss: 0.0257 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 47/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 181.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9637 | Loss: 0.0263 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 48/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:55<00:00, 181.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9590 | Loss: 0.0255 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 49/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.45it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9725 | Loss: 0.0272 | Œµ: 0.0100\n"]},{"output_type":"stream","name":"stderr","text":["Episode 50/50: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:54<00:00, 182.90it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Train Reward: 0.9632 | Loss: 0.0272 | Œµ: 0.0100\n","  Validation Reward: 0.9622\n","\n","================================================================================\n","Training Complete!\n","================================================================================\n","Best validation reward: 0.9708\n","\n"," Final model saved\n"," Training history saved\n"]}],"source":["# ============================================================================\n","# TRAINING LOOP\n","# ============================================================================\n","'''\n","print(\"\\n\" + \"=\"*80)\n","print(\"Initializing Enhanced DQN Training\")\n","print(\"=\"*80)\n","\n","# Create training environment\n","env = AppAwareMultiCloudEnv(\n","    strategic_states=strategic_states_train,\n","    data_df=train_df,\n","    app_profile_dict=app_profile_dict\n",")\n","\n","# Create validation environment (FIX: Create once, reuse)\n","val_env = AppAwareMultiCloudEnv(\n","    strategic_states=strategic_states_val,\n","    data_df=val_df,\n","    app_profile_dict=app_profile_dict\n",")\n","\n","print(f\"\\n Training environment state dimension: {env.state_dim}\")\n","print(f\" Training environment action dimension: {env.action_dim}\")\n","print(f\" Validation environment ready: {len(val_env.strategic_states):,} samples\")\n","'''\n","# Create agent\n","agent = DQNAgent(\n","    state_size=14,\n","    action_size=3,\n","    learning_rate=0.0001,  # Using fixed learning rate\n","    epsilon_decay=10000\n",")\n","\n","print(f\" Agent initialized on device: {agent.device}\")\n","print(f\" Network parameters: {sum(p.numel() for p in agent.q_network.parameters()):,}\")\n","\n","# Training configuration\n","NUM_EPISODES = 50\n","VALIDATE_EVERY = 5\n","SAMPLES_PER_EPISODE = 10000\n","\n","print(f\"\\n Training Configuration:\")\n","print(f\"   Episodes: {NUM_EPISODES}\")\n","print(f\"   Samples per episode: {SAMPLES_PER_EPISODE:,}\")\n","print(f\"   Validation frequency: every {VALIDATE_EVERY} episodes\")\n","\n","# Training metrics\n","training_history = {\n","    'episode': [],\n","    'train_reward': [],\n","    'train_loss': [],\n","    'val_reward': [],\n","    'epsilon': [],\n","    'best_val_reward': -float('inf')\n","}\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"Starting Training...\")\n","print(\"=\"*80)\n","\n","for episode in range(NUM_EPISODES):\n","    # Sample indices for this episode\n","    episode_indices = np.random.choice(len(train_df), SAMPLES_PER_EPISODE, replace=False)\n","\n","    episode_rewards = []\n","    episode_losses = []\n","\n","    # Training loop\n","    for idx in tqdm(episode_indices, desc=f\"Episode {episode+1}/{NUM_EPISODES}\"):\n","        # Get state\n","        state, row = env.reset(idx)\n","\n","        # Select action\n","        action = agent.act(state, training=True)\n","\n","        # Environment step\n","        reward, done = env.step(action, row)\n","\n","        # Get next state (if available)\n","        if idx < len(train_df) - 1:\n","            next_state, _ = env.reset(idx + 1)\n","        else:\n","            next_state = state\n","            done = True\n","\n","        # Store transition\n","        agent.remember(state, action, reward, next_state, done)\n","\n","        # Train\n","        loss = agent.train_step()\n","\n","        episode_rewards.append(reward)\n","        if loss is not None:\n","            episode_losses.append(loss)\n","\n","    # Episode statistics\n","    avg_reward = np.mean(episode_rewards)\n","    avg_loss = np.mean(episode_losses) if episode_losses else 0.0\n","\n","    training_history['episode'].append(episode + 1)\n","    training_history['train_reward'].append(avg_reward)\n","    training_history['train_loss'].append(avg_loss)\n","    training_history['epsilon'].append(agent.epsilon)\n","\n","    print(f\"\\n  Train Reward: {avg_reward:.4f} | Loss: {avg_loss:.4f} | Œµ: {agent.epsilon:.4f}\")\n","\n","    # Validation (FIXED)\n","    if (episode + 1) % VALIDATE_EVERY == 0:\n","        val_rewards = []\n","        val_indices = np.random.choice(len(val_df), min(2000, len(val_df)), replace=False)\n","\n","        for idx in val_indices:\n","            # FIX: Use val_env instead of creating new environment\n","            state, row = val_env.reset(idx)\n","            action = agent.act(state, training=False)\n","            # FIX: Use val_env.step instead of env.step\n","            reward, _ = val_env.step(action, row)\n","            val_rewards.append(reward)\n","\n","        avg_val_reward = np.mean(val_rewards)\n","        training_history['val_reward'].append(avg_val_reward)\n","\n","        print(f\"  Validation Reward: {avg_val_reward:.4f}\")\n","\n","        # Save best model\n","        if avg_val_reward > training_history['best_val_reward']:\n","            training_history['best_val_reward'] = avg_val_reward\n","            torch.save(agent.q_network.state_dict(), '/content/drive/MyDrive/mythesis/rohit-thesis/models/dqn_strategic/best_enhanced_dqn.pt')\n","            print(f\"  New best model saved!\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"Training Complete!\")\n","print(\"=\"*80)\n","print(f\"Best validation reward: {training_history['best_val_reward']:.4f}\")\n","\n","# Save final model and training history\n","torch.save(agent.q_network.state_dict(), '/content/drive/MyDrive/mythesis/rohit-thesis/models/dqn_strategic/final_enhanced_dqn.pt')\n","\n","import json\n","with open('/content/training_history.json', 'w') as f:\n","    json.dump(training_history, f, indent=2)\n","\n","print(\"\\n Final model saved\")\n","print(\" Training history saved\")"]}]}