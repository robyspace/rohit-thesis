{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: PPO Agent for Tactical Function Placement\n",
        "## Multi-Cloud Serverless Orchestration Research\n",
        "\n",
        "**Author:** Rohit  \n",
        "**Research Context:** MSc Thesis - Multi-Objective Optimization for Multi-Cloud Serverless Orchestration  \n",
        "**Phase:** 3 of 4  \n",
        "**Integration:** Builds on Phase 2 strategic cloud selection decisions  \n",
        "\n",
        "---\n",
        "\n",
        "### Objectives\n",
        "1. Implement Proximal Policy Optimization (PPO) architecture for function placement decisions\n",
        "2. Design tactical state space with 7 features capturing data locality and cold start metrics\n",
        "3. Integrate with Phase 2 strategic layer decisions for coherent multi-level optimization\n",
        "4. Optimize placement across 24 actions (4 regions × 6 memory tiers)\n",
        "5. Evaluate cold start mitigation effectiveness and data transfer cost reduction\n",
        "6. Generate comparative analysis against greedy placement and random baselines\n",
        "\n",
        "### Tactical Layer Overview\n",
        "- **State Space:** 7 features (duration, memory, invocation rate, cold start rate, avg duration, std duration, is_bursty)\n",
        "- **Action Space:** 24 discrete actions (4 regions × 6 memory tiers)\n",
        "- **Decision Frequency:** Medium-term tactical adjustments\n",
        "- **Integration:** Receives strategic cloud provider from DQN agent\n",
        "- **Optimization Focus:** Data locality, cold start minimization, inter-region communication costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "PHASE 3: PPO Tactical Layer for Function Placement\n",
        "Integrates with Phase 2 Strategic DQN Agent\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import json\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import deque\n",
        "import os\n",
        "\n",
        "# Set random seeds\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Phase 3: PPO Tactical Function Placement\")\n",
        "print(\"Multi-Cloud Serverless Orchestration Research\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Load Phase 1 & Phase 2 Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Loading Phase 1 & Phase 2 Data\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/mythesis/rohit-thesis/datasets/processed'\n",
        "MODEL_PATH = '/content/drive/MyDrive/mythesis/rohit-thesis/models/dqn_strategic'\n",
        "\n",
        "# Load Phase 1 processed data\n",
        "print(\"\\n[1/6] Loading datasets...\")\n",
        "train_df = pd.read_parquet(f'{DATA_PATH}/train_data.parquet')\n",
        "val_df = pd.read_parquet(f'{DATA_PATH}/val_data.parquet')\n",
        "test_df = pd.read_parquet(f'{DATA_PATH}/test_data.parquet')\n",
        "\n",
        "print(f\"  Train samples: {len(train_df):,}\")\n",
        "print(f\"  Val samples: {len(val_df):,}\")\n",
        "print(f\"  Test samples: {len(test_df):,}\")\n",
        "\n",
        "# Load DRL states (corrected version)\n",
        "print(\"\\n[2/6] Loading DRL state representations...\")\n",
        "drl_data = np.load(f'{DATA_PATH}/drl_states_actions_CORRECTED.npz', allow_pickle=True)\n",
        "strategic_states_full = drl_data['strategic_states']\n",
        "action_spaces = drl_data['action_spaces'].item()\n",
        "\n",
        "# Split strategic states\n",
        "train_size = len(train_df)\n",
        "val_size = len(val_df)\n",
        "\n",
        "strategic_states_train = strategic_states_full[:train_size]\n",
        "strategic_states_val = strategic_states_full[train_size:train_size+val_size]\n",
        "strategic_states_test = strategic_states_full[train_size+val_size:]\n",
        "\n",
        "print(f\"  Strategic states (train): {strategic_states_train.shape}\")\n",
        "print(f\"  Strategic states (val): {strategic_states_val.shape}\")\n",
        "\n",
        "# Load application profiles\n",
        "print(\"\\n[3/6] Loading application profiles...\")\n",
        "app_profiles = pd.read_csv(f'{DATA_PATH}/application_profiles.csv')\n",
        "app_profile_dict = app_profiles.set_index('app').to_dict('index')\n",
        "print(f\"  Application profiles: {len(app_profiles)} apps\")\n",
        "\n",
        "# Load metadata\n",
        "print(\"\\n[4/6] Loading metadata...\")\n",
        "with open(f'{DATA_PATH}/metadata.json', 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "tactical_config = metadata['drl_config']\n",
        "print(f\"  Tactical state dim: {tactical_config['tactical_state_dim']}\")\n",
        "print(f\"  Tactical action dim: {tactical_config['tactical_actions']}\")\n",
        "\n",
        "# Load scaler\n",
        "print(\"\\n[5/6] Loading feature scaler...\")\n",
        "with open(f'{DATA_PATH}/robust_scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# Extract tactical state features from metadata\n",
        "print(\"\\n[6/6] Extracting tactical state features...\")\n",
        "tactical_state_features = ['duration', 'memory_mb', 'invocation_rate', \n",
        "                           'cold_start_rate', 'avg_duration', 'std_duration', 'is_bursty']\n",
        "\n",
        "# Create tactical states from full dataset\n",
        "full_df = pd.concat([train_df, val_df, test_df], axis=0, ignore_index=True)\n",
        "\n",
        "# Check for missing features\n",
        "missing_features = [f for f in tactical_state_features if f not in full_df.columns]\n",
        "if missing_features:\n",
        "    print(f\"  Warning: Missing features {missing_features}\")\n",
        "    print(f\"  Available: {list(full_df.columns)}\")\n",
        "else:\n",
        "    print(f\"  ✓ All tactical features present\")\n",
        "\n",
        "# Extract tactical states\n",
        "tactical_states_full = full_df[tactical_state_features].values\n",
        "tactical_states_full = np.nan_to_num(tactical_states_full, nan=0.0)\n",
        "\n",
        "# Split tactical states\n",
        "tactical_states_train = tactical_states_full[:train_size]\n",
        "tactical_states_val = tactical_states_full[train_size:train_size+val_size]\n",
        "tactical_states_test = tactical_states_full[train_size+val_size:]\n",
        "\n",
        "print(f\"  Tactical states (train): {tactical_states_train.shape}\")\n",
        "print(f\"  Tactical states (val): {tactical_states_val.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Phase 1 & Phase 2 Data Loaded Successfully\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Regional Placement Action Space Design\n",
        "\n",
        "### Action Space Structure\n",
        "24 discrete actions = 4 regions × 6 memory tiers\n",
        "\n",
        "**Regions:**\n",
        "- us-east-1 (Virginia)\n",
        "- us-west-2 (Oregon)\n",
        "- eu-west-1 (Ireland)\n",
        "- ap-southeast-1 (Singapore)\n",
        "\n",
        "**Memory Tiers (MB):**\n",
        "128, 256, 512, 1024, 2048, 3008"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define action space\n",
        "REGIONS = ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-southeast-1']\n",
        "MEMORY_TIERS = [128, 256, 512, 1024, 2048, 3008]\n",
        "\n",
        "# Region characteristics\n",
        "REGION_LATENCY = {\n",
        "    'us-east-1': 0.0,      # Baseline\n",
        "    'us-west-2': 60.0,     # Cross-US latency\n",
        "    'eu-west-1': 80.0,     # Transatlantic\n",
        "    'ap-southeast-1': 180.0 # Asia-Pacific\n",
        "}\n",
        "\n",
        "REGION_CARBON_INTENSITY = {\n",
        "    'us-east-1': 385,      # gCO2/kWh\n",
        "    'us-west-2': 275,      # Lower (more renewable)\n",
        "    'eu-west-1': 295,\n",
        "    'ap-southeast-1': 525  # Higher\n",
        "}\n",
        "\n",
        "# Data locality simulation (distance to data sources)\n",
        "REGION_DATA_LOCALITY_SCORE = {\n",
        "    'us-east-1': 1.0,      # Primary data center\n",
        "    'us-west-2': 0.8,      # Secondary US\n",
        "    'eu-west-1': 0.5,      # EU replica\n",
        "    'ap-southeast-1': 0.3  # APAC replica\n",
        "}\n",
        "\n",
        "# Create action to (region, memory) mapping\n",
        "action_to_config = {}\n",
        "action_idx = 0\n",
        "for region in REGIONS:\n",
        "    for memory in MEMORY_TIERS:\n",
        "        action_to_config[action_idx] = (region, memory)\n",
        "        action_idx += 1\n",
        "\n",
        "print(\"\\nAction Space Configuration:\")\n",
        "print(f\"  Total actions: {len(action_to_config)}\")\n",
        "print(f\"  Regions: {len(REGIONS)}\")\n",
        "print(f\"  Memory tiers: {len(MEMORY_TIERS)}\")\n",
        "print(f\"\\nSample actions:\")\n",
        "for i in [0, 5, 12, 23]:\n",
        "    region, memory = action_to_config[i]\n",
        "    print(f\"  Action {i:2d}: {region:15s} | {memory:4d} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Tactical Placement Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TacticalPlacementEnv:\n",
        "    \"\"\"\n",
        "    Tactical environment for function placement decisions\n",
        "    \n",
        "    Integrates with strategic cloud selection from Phase 2\n",
        "    Optimizes regional placement and memory allocation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, tactical_states, strategic_states, data_df, \n",
        "                 app_profile_dict, action_to_config):\n",
        "        self.tactical_states = tactical_states\n",
        "        self.strategic_states = strategic_states\n",
        "        self.data_df = data_df\n",
        "        self.app_profile_dict = app_profile_dict\n",
        "        self.action_to_config = action_to_config\n",
        "        \n",
        "        self.state_dim = 11  # 7 tactical + 4 strategic context\n",
        "        self.action_dim = 24\n",
        "        \n",
        "        # Reward weights\n",
        "        self.alpha = 0.4  # Cost\n",
        "        self.beta = 0.4   # Performance\n",
        "        self.gamma = 0.2  # Carbon\n",
        "        \n",
        "    def reset(self, idx, strategic_cloud=None):\n",
        "        \"\"\"\n",
        "        Initialize state for invocation idx\n",
        "        \n",
        "        Args:\n",
        "            idx: Index in dataset\n",
        "            strategic_cloud: Cloud provider from strategic layer (0=AWS, 1=Azure, 2=GCP)\n",
        "        \n",
        "        Returns:\n",
        "            state: Enhanced tactical state (11-dim)\n",
        "            row: Data row\n",
        "        \"\"\"\n",
        "        row = self.data_df.iloc[idx]\n",
        "        \n",
        "        # Tactical state (7 features)\n",
        "        tactical_state = self.tactical_states[idx]\n",
        "        \n",
        "        # Strategic context (4 features)\n",
        "        if strategic_cloud is None:\n",
        "            # Simulate strategic decision based on app hash\n",
        "            strategic_cloud = hash(row['app']) % 3\n",
        "        \n",
        "        # One-hot encode cloud provider\n",
        "        cloud_encoding = np.zeros(3, dtype=np.float32)\n",
        "        cloud_encoding[strategic_cloud] = 1.0\n",
        "        \n",
        "        # Current region (from data)\n",
        "        current_region = row.get('region', 'us-east-1')\n",
        "        region_idx = REGIONS.index(current_region) if current_region in REGIONS else 0\n",
        "        \n",
        "        strategic_context = np.array([\n",
        "            cloud_encoding[0],  # AWS\n",
        "            cloud_encoding[1],  # Azure\n",
        "            cloud_encoding[2],  # GCP\n",
        "            region_idx / len(REGIONS)  # Normalized region\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        # Combined state\n",
        "        state = np.concatenate([tactical_state, strategic_context])\n",
        "        \n",
        "        return state, row\n",
        "    \n",
        "    def step(self, action, row):\n",
        "        \"\"\"\n",
        "        Execute tactical placement action\n",
        "        \n",
        "        Args:\n",
        "            action: Placement decision (0-23)\n",
        "            row: Current invocation data\n",
        "        \n",
        "        Returns:\n",
        "            reward: Tactical placement reward\n",
        "            done: Episode termination\n",
        "        \"\"\"\n",
        "        # Decode action\n",
        "        target_region, target_memory = self.action_to_config[action]\n",
        "        \n",
        "        # Get current configuration\n",
        "        current_region = row.get('region', 'us-east-1')\n",
        "        current_memory = row.get('memory_mb', 512)\n",
        "        \n",
        "        # Base metrics from data\n",
        "        base_cost = row.get('total_cost', 0.0)\n",
        "        base_latency = row.get('total_latency_ms', 0.0)\n",
        "        base_carbon = row.get('carbon_footprint_g', 0.0)\n",
        "        is_cold_start = row.get('is_cold_start', 0)\n",
        "        \n",
        "        # === Cost Component ===\n",
        "        # Memory cost adjustment\n",
        "        memory_cost_factor = target_memory / current_memory\n",
        "        adjusted_cost = base_cost * memory_cost_factor\n",
        "        \n",
        "        # Data transfer cost (if region changes)\n",
        "        if target_region != current_region:\n",
        "            # Add cross-region transfer penalty\n",
        "            data_transfer_penalty = 0.1 * (1.0 - REGION_DATA_LOCALITY_SCORE[target_region])\n",
        "            adjusted_cost += data_transfer_penalty\n",
        "        \n",
        "        cost_reward = 1.0 - min(adjusted_cost / 1.0, 1.0)\n",
        "        \n",
        "        # === Performance Component ===\n",
        "        # Network latency penalty\n",
        "        network_penalty = REGION_LATENCY[target_region]\n",
        "        adjusted_latency = base_latency + network_penalty\n",
        "        \n",
        "        # Cold start mitigation bonus\n",
        "        if is_cold_start and target_memory >= 1024:\n",
        "            # Higher memory reduces cold start impact\n",
        "            adjusted_latency *= 0.8\n",
        "        \n",
        "        perf_reward = 1.0 - min(adjusted_latency / 1000.0, 1.0)\n",
        "        \n",
        "        # === Carbon Component ===\n",
        "        # Region carbon intensity\n",
        "        carbon_intensity_factor = REGION_CARBON_INTENSITY[target_region] / 385.0  # Normalize\n",
        "        adjusted_carbon = base_carbon * carbon_intensity_factor\n",
        "        \n",
        "        carbon_reward = 1.0 - min(adjusted_carbon / 100.0, 1.0)\n",
        "        \n",
        "        # === Data Locality Bonus ===\n",
        "        locality_bonus = REGION_DATA_LOCALITY_SCORE[target_region] * 0.1\n",
        "        \n",
        "        # === Multi-objective reward ===\n",
        "        reward = (self.alpha * cost_reward + \n",
        "                 self.beta * perf_reward + \n",
        "                 self.gamma * carbon_reward +\n",
        "                 locality_bonus)\n",
        "        \n",
        "        # SLA penalty\n",
        "        if adjusted_latency > 1000:\n",
        "            reward -= 2.0\n",
        "        \n",
        "        return reward, False\n",
        "    \n",
        "    def evaluate_placement(self, action, row):\n",
        "        \"\"\"\n",
        "        Detailed evaluation for analysis\n",
        "        \n",
        "        Returns dict with breakdown of metrics\n",
        "        \"\"\"\n",
        "        target_region, target_memory = self.action_to_config[action]\n",
        "        current_region = row.get('region', 'us-east-1')\n",
        "        \n",
        "        reward, _ = self.step(action, row)\n",
        "        \n",
        "        return {\n",
        "            'reward': reward,\n",
        "            'target_region': target_region,\n",
        "            'target_memory': target_memory,\n",
        "            'current_region': current_region,\n",
        "            'region_changed': target_region != current_region,\n",
        "            'data_locality': REGION_DATA_LOCALITY_SCORE[target_region],\n",
        "            'carbon_intensity': REGION_CARBON_INTENSITY[target_region]\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: PPO Actor-Critic Network Architecture\n",
        "\n",
        "### Architecture Design\n",
        "- **Shared Feature Extraction:** 11 → 128 (tactical state encoder)\n",
        "- **Actor Network:** 128 → 128 → 64 → 24 (policy distribution)\n",
        "- **Critic Network:** 128 → 64 → 1 (value estimate)\n",
        "- **Activation:** ReLU with LayerNorm\n",
        "- **Output:** Categorical distribution over 24 actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOActorCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    PPO Actor-Critic with shared feature extraction\n",
        "    \n",
        "    Architecture:\n",
        "        Shared: 11 → 128\n",
        "        Actor: 128 → 128 → 64 → 24\n",
        "        Critic: 128 → 64 → 1\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim=11, action_dim=24, hidden_dim=128):\n",
        "        super(PPOActorCritic, self).__init__()\n",
        "        \n",
        "        # Shared feature extractor\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "        \n",
        "        # Actor network (policy)\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Critic network (value function)\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.LayerNorm(64),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        \n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
        "                nn.init.constant_(module.bias, 0.0)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        \n",
        "        Returns:\n",
        "            action_logits: Unnormalized action probabilities\n",
        "            value: State value estimate\n",
        "        \"\"\"\n",
        "        shared_features = self.shared(state)\n",
        "        action_logits = self.actor(shared_features)\n",
        "        value = self.critic(shared_features)\n",
        "        return action_logits, value\n",
        "    \n",
        "    def act(self, state, deterministic=False):\n",
        "        \"\"\"\n",
        "        Sample action from policy\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            deterministic: If True, return argmax action\n",
        "        \n",
        "        Returns:\n",
        "            action: Sampled action\n",
        "            log_prob: Log probability of action\n",
        "            value: State value estimate\n",
        "        \"\"\"\n",
        "        action_logits, value = self.forward(state)\n",
        "        \n",
        "        # Create categorical distribution\n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "        dist = Categorical(action_probs)\n",
        "        \n",
        "        if deterministic:\n",
        "            action = action_probs.argmax(dim=-1)\n",
        "        else:\n",
        "            action = dist.sample()\n",
        "        \n",
        "        log_prob = dist.log_prob(action)\n",
        "        \n",
        "        return action, log_prob, value\n",
        "    \n",
        "    def evaluate(self, state, action):\n",
        "        \"\"\"\n",
        "        Evaluate action under current policy\n",
        "        \n",
        "        Returns:\n",
        "            log_prob: Log probability of action\n",
        "            value: State value\n",
        "            entropy: Policy entropy\n",
        "        \"\"\"\n",
        "        action_logits, value = self.forward(state)\n",
        "        \n",
        "        action_probs = torch.softmax(action_logits, dim=-1)\n",
        "        dist = Categorical(action_probs)\n",
        "        \n",
        "        log_prob = dist.log_prob(action)\n",
        "        entropy = dist.entropy()\n",
        "        \n",
        "        return log_prob, value, entropy\n",
        "\n",
        "# Test network\n",
        "test_net = PPOActorCritic(state_dim=11, action_dim=24)\n",
        "print(f\"\\nPPO Actor-Critic Network:\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in test_net.parameters()):,}\")\n",
        "print(f\"  Actor parameters: {sum(p.numel() for p in test_net.actor.parameters()):,}\")\n",
        "print(f\"  Critic parameters: {sum(p.numel() for p in test_net.critic.parameters()):,}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_state = torch.randn(1, 11)\n",
        "test_action, test_log_prob, test_value = test_net.act(test_state)\n",
        "print(f\"\\n  Test output:\")\n",
        "print(f\"    Action: {test_action.item()}\")\n",
        "print(f\"    Log prob: {test_log_prob.item():.4f}\")\n",
        "print(f\"    Value: {test_value.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: PPO Algorithm Implementation\n",
        "\n",
        "### Key Components\n",
        "1. **Clipped Surrogate Objective:** Prevents large policy updates (ε=0.2)\n",
        "2. **Generalized Advantage Estimation (GAE):** Balances bias-variance (λ=0.95)\n",
        "3. **Multiple Epochs:** 10 epochs per batch for sample efficiency\n",
        "4. **Entropy Regularization:** Encourages exploration (coefficient=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RolloutBuffer:\n",
        "    \"\"\"\n",
        "    Storage for PPO rollout data\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.log_probs = []\n",
        "        self.rewards = []\n",
        "        self.values = []\n",
        "        self.dones = []\n",
        "    \n",
        "    def add(self, state, action, log_prob, reward, value, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.rewards.append(reward)\n",
        "        self.values.append(value)\n",
        "        self.dones.append(done)\n",
        "    \n",
        "    def compute_gae(self, last_value, gamma=0.99, gae_lambda=0.95):\n",
        "        \"\"\"\n",
        "        Compute Generalized Advantage Estimation\n",
        "        \"\"\"\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "        \n",
        "        # Backward computation\n",
        "        values = self.values + [last_value]\n",
        "        \n",
        "        for t in reversed(range(len(self.rewards))):\n",
        "            delta = self.rewards[t] + gamma * values[t+1] * (1 - self.dones[t]) - values[t]\n",
        "            gae = delta + gamma * gae_lambda * (1 - self.dones[t]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "        \n",
        "        # Returns = advantages + values\n",
        "        returns = [adv + val for adv, val in zip(advantages, self.values)]\n",
        "        \n",
        "        return advantages, returns\n",
        "    \n",
        "    def get(self):\n",
        "        return (self.states, self.actions, self.log_probs, \n",
        "                self.rewards, self.values, self.dones)\n",
        "    \n",
        "    def clear(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.log_probs.clear()\n",
        "        self.rewards.clear()\n",
        "        self.values.clear()\n",
        "        self.dones.clear()\n",
        "\n",
        "\n",
        "class PPOAgent:\n",
        "    \"\"\"\n",
        "    PPO Agent for tactical function placement\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim=11, action_dim=24, lr=3e-4, gamma=0.99,\n",
        "                 gae_lambda=0.95, clip_epsilon=0.2, vf_coef=0.5, \n",
        "                 entropy_coef=0.01, max_grad_norm=0.5):\n",
        "        \n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.clip_epsilon = clip_epsilon\n",
        "        self.vf_coef = vf_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        \n",
        "        # Network\n",
        "        self.policy = PPOActorCritic(state_dim, action_dim).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        \n",
        "        # Rollout buffer\n",
        "        self.buffer = RolloutBuffer()\n",
        "        \n",
        "        # Tracking\n",
        "        self.policy_losses = []\n",
        "        self.value_losses = []\n",
        "        self.entropy_losses = []\n",
        "    \n",
        "    def select_action(self, state, deterministic=False):\n",
        "        \"\"\"\n",
        "        Select action using current policy\n",
        "        \"\"\"\n",
        "        self.policy.eval()\n",
        "        \n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            action, log_prob, value = self.policy.act(state_tensor, deterministic)\n",
        "        \n",
        "        self.policy.train()\n",
        "        \n",
        "        return action.item(), log_prob.item(), value.item()\n",
        "    \n",
        "    def store_transition(self, state, action, log_prob, reward, value, done):\n",
        "        \"\"\"\n",
        "        Store transition in buffer\n",
        "        \"\"\"\n",
        "        self.buffer.add(state, action, log_prob, reward, value, done)\n",
        "    \n",
        "    def update(self, num_epochs=10, batch_size=64):\n",
        "        \"\"\"\n",
        "        Update policy using PPO\n",
        "        \"\"\"\n",
        "        # Get data from buffer\n",
        "        states, actions, old_log_probs, rewards, values, dones = self.buffer.get()\n",
        "        \n",
        "        if len(states) == 0:\n",
        "            return None\n",
        "        \n",
        "        # Compute advantages and returns\n",
        "        last_value = values[-1] if len(values) > 0 else 0.0\n",
        "        advantages, returns = self.buffer.compute_gae(last_value, self.gamma, self.gae_lambda)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states_tensor = torch.FloatTensor(np.array(states)).to(self.device)\n",
        "        actions_tensor = torch.LongTensor(actions).to(self.device)\n",
        "        old_log_probs_tensor = torch.FloatTensor(old_log_probs).to(self.device)\n",
        "        advantages_tensor = torch.FloatTensor(advantages).to(self.device)\n",
        "        returns_tensor = torch.FloatTensor(returns).to(self.device)\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages_tensor = (advantages_tensor - advantages_tensor.mean()) / (advantages_tensor.std() + 1e-8)\n",
        "        \n",
        "        # Multiple epochs of updates\n",
        "        total_policy_loss = 0\n",
        "        total_value_loss = 0\n",
        "        total_entropy_loss = 0\n",
        "        \n",
        "        dataset_size = len(states)\n",
        "        \n",
        "        for epoch in range(num_epochs):\n",
        "            # Shuffle indices\n",
        "            indices = np.random.permutation(dataset_size)\n",
        "            \n",
        "            # Mini-batch updates\n",
        "            for start in range(0, dataset_size, batch_size):\n",
        "                end = min(start + batch_size, dataset_size)\n",
        "                batch_indices = indices[start:end]\n",
        "                \n",
        "                # Get batch\n",
        "                batch_states = states_tensor[batch_indices]\n",
        "                batch_actions = actions_tensor[batch_indices]\n",
        "                batch_old_log_probs = old_log_probs_tensor[batch_indices]\n",
        "                batch_advantages = advantages_tensor[batch_indices]\n",
        "                batch_returns = returns_tensor[batch_indices]\n",
        "                \n",
        "                # Evaluate actions\n",
        "                log_probs, values, entropy = self.policy.evaluate(batch_states, batch_actions)\n",
        "                \n",
        "                # Policy loss (clipped surrogate objective)\n",
        "                ratio = torch.exp(log_probs - batch_old_log_probs)\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
        "                policy_loss = -torch.min(surr1, surr2).mean()\n",
        "                \n",
        "                # Value loss\n",
        "                value_loss = nn.MSELoss()(values.squeeze(), batch_returns)\n",
        "                \n",
        "                # Entropy loss (for exploration)\n",
        "                entropy_loss = -entropy.mean()\n",
        "                \n",
        "                # Total loss\n",
        "                loss = policy_loss + self.vf_coef * value_loss + self.entropy_coef * entropy_loss\n",
        "                \n",
        "                # Backpropagation\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                # Track losses\n",
        "                total_policy_loss += policy_loss.item()\n",
        "                total_value_loss += value_loss.item()\n",
        "                total_entropy_loss += entropy_loss.item()\n",
        "        \n",
        "        # Clear buffer\n",
        "        self.buffer.clear()\n",
        "        \n",
        "        # Average losses\n",
        "        num_updates = num_epochs * (dataset_size // batch_size + 1)\n",
        "        avg_policy_loss = total_policy_loss / num_updates\n",
        "        avg_value_loss = total_value_loss / num_updates\n",
        "        avg_entropy_loss = total_entropy_loss / num_updates\n",
        "        \n",
        "        self.policy_losses.append(avg_policy_loss)\n",
        "        self.value_losses.append(avg_value_loss)\n",
        "        self.entropy_losses.append(avg_entropy_loss)\n",
        "        \n",
        "        return {\n",
        "            'policy_loss': avg_policy_loss,\n",
        "            'value_loss': avg_value_loss,\n",
        "            'entropy_loss': avg_entropy_loss\n",
        "        }\n",
        "\n",
        "print(\"\\nPPO Agent Initialized\")\n",
        "print(f\"  Hyperparameters:\")\n",
        "print(f\"    Learning rate: 3e-4\")\n",
        "print(f\"    Gamma: 0.99\")\n",
        "print(f\"    GAE lambda: 0.95\")\n",
        "print(f\"    Clip epsilon: 0.2\")\n",
        "print(f\"    Entropy coefficient: 0.01\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Initialize Environments and Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Initializing Training Environment\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create environments\n",
        "train_env = TacticalPlacementEnv(\n",
        "    tactical_states=tactical_states_train,\n",
        "    strategic_states=strategic_states_train,\n",
        "    data_df=train_df,\n",
        "    app_profile_dict=app_profile_dict,\n",
        "    action_to_config=action_to_config\n",
        ")\n",
        "\n",
        "val_env = TacticalPlacementEnv(\n",
        "    tactical_states=tactical_states_val,\n",
        "    strategic_states=strategic_states_val,\n",
        "    data_df=val_df,\n",
        "    app_profile_dict=app_profile_dict,\n",
        "    action_to_config=action_to_config\n",
        ")\n",
        "\n",
        "print(f\"\\n  Training environment: {len(train_env.tactical_states):,} samples\")\n",
        "print(f\"  Validation environment: {len(val_env.tactical_states):,} samples\")\n",
        "\n",
        "# Create agent\n",
        "agent = PPOAgent(\n",
        "    state_dim=11,\n",
        "    action_dim=24,\n",
        "    lr=3e-4,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_epsilon=0.2,\n",
        "    entropy_coef=0.01\n",
        ")\n",
        "\n",
        "print(f\"\\n  Agent device: {agent.device}\")\n",
        "print(f\"  Policy parameters: {sum(p.numel() for p in agent.policy.parameters()):,}\")\n",
        "\n",
        "# Test environment\n",
        "test_state, test_row = train_env.reset(0)\n",
        "test_action, test_log_prob, test_value = agent.select_action(test_state)\n",
        "test_reward, _ = train_env.step(test_action, test_row)\n",
        "\n",
        "print(f\"\\n  Environment test:\")\n",
        "print(f\"    State shape: {test_state.shape}\")\n",
        "print(f\"    Action: {test_action} → {action_to_config[test_action]}\")\n",
        "print(f\"    Reward: {test_reward:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Environment Ready\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: PPO Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Starting PPO Training\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Training configuration\n",
        "NUM_EPISODES = 30\n",
        "ROLLOUT_LENGTH = 2048  # Collect 2048 transitions per episode\n",
        "UPDATE_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "VALIDATE_EVERY = 5\n",
        "\n",
        "print(f\"\\n  Configuration:\")\n",
        "print(f\"    Episodes: {NUM_EPISODES}\")\n",
        "print(f\"    Rollout length: {ROLLOUT_LENGTH:,}\")\n",
        "print(f\"    Update epochs: {UPDATE_EPOCHS}\")\n",
        "print(f\"    Batch size: {BATCH_SIZE}\")\n",
        "print(f\"    Validation frequency: every {VALIDATE_EVERY} episodes\")\n",
        "\n",
        "# Training history\n",
        "training_history = {\n",
        "    'episode': [],\n",
        "    'train_reward': [],\n",
        "    'train_policy_loss': [],\n",
        "    'train_value_loss': [],\n",
        "    'val_reward': [],\n",
        "    'best_val_reward': -float('inf')\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training Progress\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for episode in range(NUM_EPISODES):\n",
        "    # Sample indices for rollout\n",
        "    rollout_indices = np.random.choice(len(train_df), ROLLOUT_LENGTH, replace=False)\n",
        "    \n",
        "    episode_rewards = []\n",
        "    \n",
        "    # Collect rollout\n",
        "    for idx in tqdm(rollout_indices, desc=f\"Episode {episode+1}/{NUM_EPISODES} - Collecting\"):\n",
        "        # Get state\n",
        "        state, row = train_env.reset(idx)\n",
        "        \n",
        "        # Select action\n",
        "        action, log_prob, value = agent.select_action(state)\n",
        "        \n",
        "        # Environment step\n",
        "        reward, done = train_env.step(action, row)\n",
        "        \n",
        "        # Store transition\n",
        "        agent.store_transition(state, action, log_prob, reward, value, done)\n",
        "        \n",
        "        episode_rewards.append(reward)\n",
        "    \n",
        "    # Update policy\n",
        "    update_info = agent.update(num_epochs=UPDATE_EPOCHS, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    # Episode statistics\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "    \n",
        "    training_history['episode'].append(episode + 1)\n",
        "    training_history['train_reward'].append(avg_reward)\n",
        "    \n",
        "    if update_info:\n",
        "        training_history['train_policy_loss'].append(update_info['policy_loss'])\n",
        "        training_history['train_value_loss'].append(update_info['value_loss'])\n",
        "        \n",
        "        print(f\"\\n  Ep {episode+1:2d} | Reward: {avg_reward:.4f} | \"\n",
        "              f\"Policy Loss: {update_info['policy_loss']:.4f} | \"\n",
        "              f\"Value Loss: {update_info['value_loss']:.4f}\")\n",
        "    else:\n",
        "        print(f\"\\n  Ep {episode+1:2d} | Reward: {avg_reward:.4f}\")\n",
        "    \n",
        "    # Validation\n",
        "    if (episode + 1) % VALIDATE_EVERY == 0:\n",
        "        val_rewards = []\n",
        "        val_indices = np.random.choice(len(val_df), min(1000, len(val_df)), replace=False)\n",
        "        \n",
        "        for idx in val_indices:\n",
        "            state, row = val_env.reset(idx)\n",
        "            action, _, _ = agent.select_action(state, deterministic=True)\n",
        "            reward, _ = val_env.step(action, row)\n",
        "            val_rewards.append(reward)\n",
        "        \n",
        "        avg_val_reward = np.mean(val_rewards)\n",
        "        training_history['val_reward'].append(avg_val_reward)\n",
        "        \n",
        "        print(f\"  Validation Reward: {avg_val_reward:.4f}\")\n",
        "        \n",
        "        # Save best model\n",
        "        if avg_val_reward > training_history['best_val_reward']:\n",
        "            training_history['best_val_reward'] = avg_val_reward\n",
        "            \n",
        "            os.makedirs('/content/drive/MyDrive/mythesis/rohit-thesis/models/ppo_tactical', exist_ok=True)\n",
        "            torch.save(agent.policy.state_dict(), \n",
        "                      '/content/drive/MyDrive/mythesis/rohit-thesis/models/ppo_tactical/best_ppo_tactical.pt')\n",
        "            print(f\"  ✓ New best model saved!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Training Complete\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Best validation reward: {training_history['best_val_reward']:.4f}\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(agent.policy.state_dict(), \n",
        "          '/content/drive/MyDrive/mythesis/rohit-thesis/models/ppo_tactical/final_ppo_tactical.pt')\n",
        "\n",
        "# Save training history\n",
        "with open('/content/ppo_training_history.json', 'w') as f:\n",
        "    json.dump(training_history, f, indent=2)\n",
        "\n",
        "print(\"\\n  ✓ Final model saved\")\n",
        "print(\"  ✓ Training history saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# 1. Training reward\n",
        "axes[0, 0].plot(training_history['episode'], training_history['train_reward'], \n",
        "                marker='o', linewidth=2, markersize=6, label='Train Reward')\n",
        "axes[0, 0].set_title('Training Reward Progress', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].set_xlabel('Episode')\n",
        "axes[0, 0].set_ylabel('Average Reward')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 2. Policy loss\n",
        "if training_history['train_policy_loss']:\n",
        "    axes[0, 1].plot(training_history['episode'], training_history['train_policy_loss'], \n",
        "                    marker='s', linewidth=2, markersize=6, color='red', label='Policy Loss')\n",
        "    axes[0, 1].set_title('Policy Loss', fontweight='bold', fontsize=12)\n",
        "    axes[0, 1].set_xlabel('Episode')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "# 3. Value loss\n",
        "if training_history['train_value_loss']:\n",
        "    axes[1, 0].plot(training_history['episode'], training_history['train_value_loss'], \n",
        "                    marker='^', linewidth=2, markersize=6, color='green', label='Value Loss')\n",
        "    axes[1, 0].set_title('Value Function Loss', fontweight='bold', fontsize=12)\n",
        "    axes[1, 0].set_xlabel('Episode')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].legend()\n",
        "\n",
        "# 4. Validation reward\n",
        "if training_history['val_reward']:\n",
        "    val_episodes = [i * VALIDATE_EVERY for i in range(1, len(training_history['val_reward']) + 1)]\n",
        "    axes[1, 1].plot(val_episodes, training_history['val_reward'], \n",
        "                    marker='D', linewidth=2, markersize=8, color='purple', label='Validation Reward')\n",
        "    axes[1, 1].axhline(y=training_history['best_val_reward'], color='red', \n",
        "                       linestyle='--', linewidth=2, label=f\"Best: {training_history['best_val_reward']:.4f}\")\n",
        "    axes[1, 1].set_title('Validation Performance', fontweight='bold', fontsize=12)\n",
        "    axes[1, 1].set_xlabel('Episode')\n",
        "    axes[1, 1].set_ylabel('Average Reward')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "plt.suptitle('PPO Tactical Layer Training Progress', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/mythesis/rohit-thesis/outputs/ppo_training_progress.png', \n",
        "            dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Training visualization saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Baseline Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Baseline Comparisons\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def evaluate_policy(env, agent, num_samples=1000, deterministic=True, policy_type='ppo'):\n",
        "    \"\"\"\n",
        "    Evaluate a policy on environment\n",
        "    \n",
        "    Args:\n",
        "        policy_type: 'ppo', 'random', 'greedy_locality', 'greedy_cost'\n",
        "    \"\"\"\n",
        "    indices = np.random.choice(len(env.data_df), min(num_samples, len(env.data_df)), replace=False)\n",
        "    \n",
        "    rewards = []\n",
        "    actions = []\n",
        "    region_selections = []\n",
        "    memory_selections = []\n",
        "    \n",
        "    for idx in tqdm(indices, desc=f\"Evaluating {policy_type}\"):\n",
        "        state, row = env.reset(idx)\n",
        "        \n",
        "        if policy_type == 'ppo':\n",
        "            action, _, _ = agent.select_action(state, deterministic=deterministic)\n",
        "        elif policy_type == 'random':\n",
        "            action = np.random.randint(0, env.action_dim)\n",
        "        elif policy_type == 'greedy_locality':\n",
        "            # Always select us-east-1 (highest data locality) with median memory\n",
        "            action = 2  # us-east-1 + 512MB\n",
        "        elif policy_type == 'greedy_cost':\n",
        "            # Always select lowest memory tier\n",
        "            action = 0  # us-east-1 + 128MB (cheapest)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown policy type: {policy_type}\")\n",
        "        \n",
        "        reward, _ = env.step(action, row)\n",
        "        \n",
        "        rewards.append(reward)\n",
        "        actions.append(action)\n",
        "        \n",
        "        region, memory = action_to_config[action]\n",
        "        region_selections.append(region)\n",
        "        memory_selections.append(memory)\n",
        "    \n",
        "    return {\n",
        "        'rewards': rewards,\n",
        "        'mean_reward': np.mean(rewards),\n",
        "        'std_reward': np.std(rewards),\n",
        "        'actions': actions,\n",
        "        'regions': region_selections,\n",
        "        'memories': memory_selections\n",
        "    }\n",
        "\n",
        "# Evaluate all policies\n",
        "print(\"\\n[1/4] Evaluating PPO agent...\")\n",
        "ppo_results = evaluate_policy(val_env, agent, num_samples=2000, policy_type='ppo')\n",
        "\n",
        "print(\"\\n[2/4] Evaluating random baseline...\")\n",
        "random_results = evaluate_policy(val_env, agent, num_samples=2000, policy_type='random')\n",
        "\n",
        "print(\"\\n[3/4] Evaluating greedy locality baseline...\")\n",
        "greedy_locality_results = evaluate_policy(val_env, agent, num_samples=2000, policy_type='greedy_locality')\n",
        "\n",
        "print(\"\\n[4/4] Evaluating greedy cost baseline...\")\n",
        "greedy_cost_results = evaluate_policy(val_env, agent, num_samples=2000, policy_type='greedy_cost')\n",
        "\n",
        "# Print comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Baseline Comparison Results\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n{'Policy':<20} {'Mean Reward':>15} {'Std Reward':>15} {'Improvement':>15}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "baseline_reward = random_results['mean_reward']\n",
        "\n",
        "for name, results in [('PPO Agent', ppo_results), \n",
        "                      ('Random', random_results),\n",
        "                      ('Greedy Locality', greedy_locality_results),\n",
        "                      ('Greedy Cost', greedy_cost_results)]:\n",
        "    improvement = ((results['mean_reward'] - baseline_reward) / abs(baseline_reward)) * 100\n",
        "    print(f\"{name:<20} {results['mean_reward']:>15.4f} {results['std_reward']:>15.4f} {improvement:>14.2f}%\")\n",
        "\n",
        "# Save results\n",
        "baseline_comparison = {\n",
        "    'ppo': ppo_results['mean_reward'],\n",
        "    'random': random_results['mean_reward'],\n",
        "    'greedy_locality': greedy_locality_results['mean_reward'],\n",
        "    'greedy_cost': greedy_cost_results['mean_reward']\n",
        "}\n",
        "\n",
        "with open('/content/baseline_comparison.json', 'w') as f:\n",
        "    json.dump(baseline_comparison, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Policy Analysis & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "\n",
        "# 1. Reward comparison\n",
        "policies = ['PPO\\nAgent', 'Random', 'Greedy\\nLocality', 'Greedy\\nCost']\n",
        "mean_rewards = [ppo_results['mean_reward'], random_results['mean_reward'],\n",
        "                greedy_locality_results['mean_reward'], greedy_cost_results['mean_reward']]\n",
        "std_rewards = [ppo_results['std_reward'], random_results['std_reward'],\n",
        "               greedy_locality_results['std_reward'], greedy_cost_results['std_reward']]\n",
        "\n",
        "colors = ['#2ecc71', '#e74c3c', '#3498db', '#f39c12']\n",
        "axes[0, 0].bar(policies, mean_rewards, color=colors, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].errorbar(policies, mean_rewards, yerr=std_rewards, fmt='none', color='black', capsize=5)\n",
        "axes[0, 0].set_title('Policy Comparison', fontweight='bold', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Mean Reward')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 2. Region distribution (PPO)\n",
        "region_counts = pd.Series(ppo_results['regions']).value_counts()\n",
        "axes[0, 1].bar(region_counts.index, region_counts.values, color='steelblue', edgecolor='black')\n",
        "axes[0, 1].set_title('PPO Region Selection Distribution', fontweight='bold', fontsize=12)\n",
        "axes[0, 1].set_xlabel('Region')\n",
        "axes[0, 1].set_ylabel('Count')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Memory distribution (PPO)\n",
        "memory_counts = pd.Series(ppo_results['memories']).value_counts().sort_index()\n",
        "axes[0, 2].bar(memory_counts.index.astype(str), memory_counts.values, color='green', edgecolor='black')\n",
        "axes[0, 2].set_title('PPO Memory Selection Distribution', fontweight='bold', fontsize=12)\n",
        "axes[0, 2].set_xlabel('Memory (MB)')\n",
        "axes[0, 2].set_ylabel('Count')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Reward distribution comparison\n",
        "axes[1, 0].hist([ppo_results['rewards'], random_results['rewards']], \n",
        "                bins=30, label=['PPO', 'Random'], alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_title('Reward Distribution Comparison', fontweight='bold', fontsize=12)\n",
        "axes[1, 0].set_xlabel('Reward')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Action heatmap (Region × Memory)\n",
        "action_matrix = np.zeros((len(REGIONS), len(MEMORY_TIERS)))\n",
        "for action in ppo_results['actions']:\n",
        "    region, memory = action_to_config[action]\n",
        "    region_idx = REGIONS.index(region)\n",
        "    memory_idx = MEMORY_TIERS.index(memory)\n",
        "    action_matrix[region_idx, memory_idx] += 1\n",
        "\n",
        "sns.heatmap(action_matrix, annot=True, fmt='.0f', cmap='YlOrRd', \n",
        "            xticklabels=MEMORY_TIERS, yticklabels=REGIONS, \n",
        "            ax=axes[1, 1], cbar_kws={'label': 'Selection Count'})\n",
        "axes[1, 1].set_title('PPO Action Heatmap (Region × Memory)', fontweight='bold', fontsize=12)\n",
        "axes[1, 1].set_xlabel('Memory Tier (MB)')\n",
        "axes[1, 1].set_ylabel('Region')\n",
        "\n",
        "# 6. Cumulative reward\n",
        "ppo_cumulative = np.cumsum(ppo_results['rewards'])\n",
        "random_cumulative = np.cumsum(random_results['rewards'])\n",
        "axes[1, 2].plot(ppo_cumulative, label='PPO', linewidth=2)\n",
        "axes[1, 2].plot(random_cumulative, label='Random', linewidth=2)\n",
        "axes[1, 2].set_title('Cumulative Reward', fontweight='bold', fontsize=12)\n",
        "axes[1, 2].set_xlabel('Evaluation Step')\n",
        "axes[1, 2].set_ylabel('Cumulative Reward')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('PPO Tactical Layer - Policy Analysis', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/MyDrive/mythesis/rohit-thesis/outputs/ppo_policy_analysis.png', \n",
        "            dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Policy analysis visualization saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 11: Phase 3 Summary & Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PHASE 3 SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n✅ ACHIEVEMENTS:\")\n",
        "print(\"  ✓ Implemented PPO Actor-Critic architecture for tactical placement\")\n",
        "print(\"  ✓ Designed 11-dim tactical state space with strategic integration\")\n",
        "print(\"  ✓ Created 24-action space (4 regions × 6 memory tiers)\")\n",
        "print(\"  ✓ Trained PPO agent with GAE and clipped surrogate objective\")\n",
        "print(f\"  ✓ Achieved {ppo_results['mean_reward']:.4f} mean reward (validation)\")\n",
        "print(f\"  ✓ Outperformed random baseline by {((ppo_results['mean_reward'] - random_results['mean_reward']) / abs(random_results['mean_reward']) * 100):.2f}%\")\n",
        "\n",
        "print(\"\\n📊 KEY FINDINGS:\")\n",
        "print(f\"  • PPO mean reward: {ppo_results['mean_reward']:.4f} ± {ppo_results['std_reward']:.4f}\")\n",
        "print(f\"  • Random baseline: {random_results['mean_reward']:.4f} ± {random_results['std_reward']:.4f}\")\n",
        "print(f\"  • Greedy locality: {greedy_locality_results['mean_reward']:.4f}\")\n",
        "print(f\"  • Greedy cost: {greedy_cost_results['mean_reward']:.4f}\")\n",
        "print(f\"  • Most selected region: {pd.Series(ppo_results['regions']).mode()[0]}\")\n",
        "print(f\"  • Most selected memory: {pd.Series(ppo_results['memories']).mode()[0]} MB\")\n",
        "\n",
        "print(\"\\n🔗 INTEGRATION NOTES:\")\n",
        "print(\"  • Phase 2 outputs consumed: strategic cloud selection context\")\n",
        "print(\"  • Phase 3 outputs generated:\")\n",
        "print(\"    - best_ppo_tactical.pt (best model)\")\n",
        "print(\"    - final_ppo_tactical.pt (final model)\")\n",
        "print(\"    - ppo_training_history.json (training metrics)\")\n",
        "print(\"    - baseline_comparison.json (baseline results)\")\n",
        "print(\"  • Ready for Phase 4: LSTM operational resource allocation\")\n",
        "\n",
        "print(\"\\n📁 FILES GENERATED:\")\n",
        "print(\"  models/ppo_tactical/\")\n",
        "print(\"    ├── best_ppo_tactical.pt\")\n",
        "    ├── final_ppo_tactical.pt\")\n",
        "print(\"  outputs/\")\n",
        "print(\"    ├── ppo_training_progress.png\")\n",
        "print(\"    └── ppo_policy_analysis.png\")\n",
        "\n",
        "print(\"\\n🎯 NEXT STEPS (Phase 4):\")\n",
        "print(\"  [ ] Implement LSTM architecture for workload prediction\")\n",
        "print(\"  [ ] Design operational state space (5 temporal features)\")\n",
        "print(\"  [ ] Implement asymmetric loss function (penalize under-provisioning)\")\n",
        "print(\"  [ ] Train LSTM with 12-step sequence (3-minute lookback)\")\n",
        "print(\"  [ ] Integrate with strategic (DQN) + tactical (PPO) layers\")\n",
        "print(\"  [ ] Conduct end-to-end hierarchical evaluation\")\n",
        "print(\"  [ ] Generate final thesis results and visualizations\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✨ PHASE 3 SUCCESSFULLY COMPLETED\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
